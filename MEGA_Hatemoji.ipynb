{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, MegaForSequenceClassification\n",
    "import torchinfo\n",
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"mnaylor/mega-base-wikitext\")\n",
    "model = MegaForSequenceClassification.from_pretrained(\n",
    "    \"mnaylor/mega-base-wikitext\")\n",
    "model.to(device)\n",
    "\n",
    "torchinfo.summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data_check_test = load_dataset(\n",
    "    \"csv\", data_files=\"datasets/Hatemoji-main/HatemojiCheck/test.csv\"\n",
    "    )\n",
    "print(data_check_test['train'][0])\n",
    "\n",
    "# Dataset only have training data, hence split it into train/test\n",
    "data_check_test = data_check_test['train'].train_test_split(test_size=0.25)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_func(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"text\"], padding=\"max_length\", truncation=True\n",
    "        )\n",
    "\n",
    "tokenized_data = data_check_test.map(tokenize_func, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare for torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "tokenized_data = tokenized_data.remove_columns([\n",
    "    \"text\", \"case_id\", \"templ_id\", \"test_group_id\", \"target\", \"functionality\", \"set\", \"unrealistic_flags\", \"included_in_test_suite\"])\n",
    "\n",
    "tokenized_data = tokenized_data.rename_column(\"label_gold\", \"labels\")\n",
    "tokenized_data.set_format(\"torch\")\n",
    "\n",
    "print(tokenized_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_dataloader = DataLoader(\n",
    "    tokenized_data['train'], batch_size=64, shuffle=True)\n",
    "test_dataloader = DataLoader(tokenized_data['test'], batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import evaluate\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import get_scheduler\n",
    "from torch.optim import AdamW\n",
    "import copy\n",
    "\n",
    "def train_epoch(model, train_dataloader, optimizer, lr_scheduler):\n",
    "    progress_bar = tqdm(range(len(train_dataloader)))\n",
    "    metric = evaluate.load(\"accuracy\")\n",
    "    model.train()\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    epoch_loss = 0\n",
    "    for batch in train_dataloader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "        outputs = model(**batch)\n",
    "        logits = outputs.logits\n",
    "        loss = criterion(logits, batch[\"labels\"])\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "        \n",
    "        metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        progress_bar.update(1)\n",
    "    \n",
    "    lr_scheduler.step()\n",
    "    epoch_loss /= len(train_dataloader)\n",
    "    accuracy = metric.compute()['accuracy']\n",
    "\n",
    "    return epoch_loss, accuracy\n",
    "\n",
    "def eval(model, test_dataloader):\n",
    "    progress_bar = tqdm(range(len(test_dataloader)))\n",
    "    metric = evaluate.load(\"accuracy\")\n",
    "    model.eval()\n",
    "    model.mega.requires_grad_(False)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    epoch_loss = 0\n",
    "    for batch in test_dataloader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "\n",
    "        logits = outputs.logits\n",
    "        loss = criterion(logits, batch[\"labels\"])\n",
    "\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "\n",
    "        metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        progress_bar.update(1)\n",
    "\n",
    "    epoch_loss /= len(test_dataloader)\n",
    "    accuracy = metric.compute()['accuracy']\n",
    "    \n",
    "    return epoch_loss, accuracy\n",
    "\n",
    "def train(model,\n",
    "          train_dataloader,\n",
    "          test_dataloader,\n",
    "          num_epochs=2,\n",
    "          learning_rate=5e-5,\n",
    "          patience=4):\n",
    "\n",
    "    # count epochs where the model didn't improve\n",
    "    counter = 0\n",
    "    best_val_acc = 0\n",
    "    best_epoch = 0\n",
    "    best_model = None\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    lr_scheduler = get_scheduler(\n",
    "        name=\"polynomial\", optimizer=optimizer, num_warmup_steps=1,     num_training_steps=num_epochs*len(train_dataloader)\n",
    "    )\n",
    "    train_accuracies = []\n",
    "    val_accuracies = []\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss, train_acc = train_epoch(\n",
    "            model, train_dataloader, optimizer, lr_scheduler)\n",
    "        val_loss, val_acc = eval(model, test_dataloader)\n",
    "\n",
    "        print(f\"Epoch {epoch+1} accuracy: train={train_acc:.3f}, test={val_acc:.3f}\")\n",
    "\n",
    "        train_accuracies.append(train_acc)\n",
    "        val_accuracies.append(val_acc)\n",
    "\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        # Unfreeze MEGA\n",
    "        if epoch >= 1:\n",
    "            model.mega.requires_grad_(True)\n",
    "            \n",
    "        # early stopping\n",
    "        if val_acc <= best_val_acc + 1e-4:\n",
    "            counter += 1\n",
    "            if counter >= patience:\n",
    "                break\n",
    "        else:\n",
    "            best_val_acc = val_acc\n",
    "            counter = 0\n",
    "            best_epoch = epoch\n",
    "            best_model = copy.deepcopy(model.state_dict())\n",
    "\n",
    "    return train_accuracies, val_accuracies, train_losses, val_losses, best_epoch, best_model\n",
    "\n",
    "model.mega.requires_grad_(False)\n",
    "N_EPOCHS = 5\n",
    "train_acc, val_acc, train_losses, val_losses, best_epoch, best_model = train(model, train_dataloader, test_dataloader, N_EPOCHS, learning_rate=1e-4)\n",
    "\n",
    "all_data = (train_acc, val_acc, train_losses, val_losses, best_epoch, best_model)\n",
    "with open(f\"mega_hatemoji.obj\", \"wb\") as f:\n",
    "    pickle.dump(all_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"mega_hatemoji.obj\", \"rb\") as f:\n",
    "    train_acc, val_acc, train_losses, val_losses, best_epoch, best_model = pickle.load(f)\n",
    "\n",
    "fig, ax = plt.subplots(1,2, figsize=(10,4))\n",
    "epoch_axis = range(1, len(train_losses)+1)\n",
    "ax[0].plot(epoch_axis, train_losses, label='train')\n",
    "ax[0].plot(epoch_axis, val_losses, label='val')\n",
    "ax[0].axvline(best_epoch+1, label='best', linestyle='--')\n",
    "ax[0].set_xlabel(\"Epoch\")\n",
    "ax[0].set_ylabel(\"Loss\")\n",
    "ax[0].set_xticks(epoch_axis)\n",
    "ax[0].legend()\n",
    "\n",
    "ax[1].plot(epoch_axis, train_acc, label='train')\n",
    "ax[1].plot(epoch_axis, val_acc, label='val')\n",
    "ax[1].axvline(best_epoch+1, label='best', linestyle='--')\n",
    "ax[1].set_xlabel(\"Epoch\")\n",
    "ax[1].set_ylabel(\"Accuracy\")\n",
    "ax[1].set_xticks(epoch_axis)\n",
    "ax[1].legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "best_train_accs = []\n",
    "best_train_losses = []\n",
    "best_val_accs = []\n",
    "best_val_losses = []\n",
    "\n",
    "best_epochs = []\n",
    "\n",
    "all_train_accs = []\n",
    "all_train_losses = []\n",
    "all_val_accs = []\n",
    "all_val_losses = []\n",
    "\n",
    "N_SEEDS = 5\n",
    "N_EPOCHS = 20\n",
    "for seed in tqdm(range(N_SEEDS)):\n",
    "    model = MegaForSequenceClassification.from_pretrained(\n",
    "    \"mnaylor/mega-base-wikitext\")\n",
    "    model.to(device)\n",
    "    model.mega.requires_grad_(False)\n",
    "      \n",
    "    train_acc, val_acc, train_losses, val_losses, best_epoch, _ = train(model, train_dataloader, test_dataloader, N_EPOCHS, learning_rate=1e-4, patience=2)\n",
    "\n",
    "    best_train_accs.append(train_acc[best_epoch])\n",
    "    best_val_accs.append(val_acc[best_epoch])\n",
    "    best_train_losses.append(train_losses[best_epoch])\n",
    "    best_val_losses.append(val_losses[best_epoch])\n",
    "    best_epochs.append(best_epoch)\n",
    "\n",
    "    all_train_accs.append(train_acc)\n",
    "    all_train_losses.append(train_losses)\n",
    "    all_val_accs.append(val_acc)\n",
    "    all_val_losses.append(val_losses)\n",
    "\n",
    "all_data_seeds = (best_train_accs, best_val_accs, best_train_losses, best_val_losses, best_epochs, all_train_accs, all_train_losses, all_val_accs, all_val_losses)\n",
    "\n",
    "with open(f\"mega_hatemoji_seeds.obj\", \"wb\") as f:\n",
    "    pickle.dump(all_data_seeds, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "with open(f\"mega_hatemoji_seeds.obj\", \"rb\") as f:\n",
    "    best_train_accs, best_val_accs, best_train_losses, best_val_losses, best_epochs, all_train_accs, all_train_losses, all_val_accs, all_val_losses = pickle.load(f)\n",
    "\n",
    "# Plot with epochs\n",
    "def aggregate_over_seeds(data, epochs):\n",
    "    means = []\n",
    "    mins = []\n",
    "    maxs = []\n",
    "    padded_data = []\n",
    "    for seed_data in data:\n",
    "        # some seeds may have stopped early\n",
    "        # hence pad with edge values\n",
    "        padded_data.append(np.pad(seed_data, (0, epochs-len(seed_data)), 'edge'))\n",
    "    for epoch in range(N_EPOCHS):\n",
    "        epoch_data = torch.Tensor([seed[epoch] for seed in padded_data])\n",
    "        means.append(torch.mean(epoch_data))\n",
    "        mins.append(torch.min(epoch_data))\n",
    "        maxs.append(torch.max(epoch_data))\n",
    "    \n",
    "    return means, mins, maxs\n",
    "\n",
    "mean_train_accs, min_train_accs, max_train_accs = aggregate_over_seeds(all_train_accs, N_EPOCHS)\n",
    "mean_val_losses, min_val_losses, max_val_losses= aggregate_over_seeds(all_val_losses, N_EPOCHS)\n",
    "mean_val_accs, min_val_accs, max_val_accs = aggregate_over_seeds(all_val_accs, N_EPOCHS)\n",
    "mean_train_losses, min_train_losses, max_train_losses = aggregate_over_seeds(all_train_losses, N_EPOCHS)\n",
    "\n",
    "os.makedirs(f\"figures/\", exist_ok=True)\n",
    "\n",
    "fig, ax = plt.subplots(1,2, figsize=(10,4))\n",
    "epoch_axis = range(1, N_EPOCHS+1)\n",
    "ax[0].plot(epoch_axis, mean_train_losses, color='C0', label='train')\n",
    "ax[0].fill_between(epoch_axis, min_train_losses, max_train_losses, color='C0', alpha=0.3)\n",
    "ax[0].plot(epoch_axis, mean_val_losses, color='C1', label='val')\n",
    "ax[0].fill_between(epoch_axis, min_val_losses, max_val_losses, color='C1', alpha=0.3)\n",
    "ax[0].axvline(best_epoch+1, label='best', linestyle='--')\n",
    "ax[0].set_xlabel(\"Epoch\")\n",
    "ax[0].set_ylabel(\"Loss\")\n",
    "#ax[0].set_xticks(epoch_axis)\n",
    "ax[0].grid()\n",
    "ax[0].legend()\n",
    "\n",
    "ax[1].plot(epoch_axis, mean_train_accs, color='C0', label='train')\n",
    "ax[1].fill_between(epoch_axis, min_train_accs, max_train_accs, color='C0', alpha=0.3)\n",
    "ax[1].plot(epoch_axis, mean_val_accs, color='C1', label='val')\n",
    "ax[1].fill_between(epoch_axis, min_val_accs, max_val_accs, color='C1', alpha=0.3)\n",
    "ax[1].axvline(best_epoch+1, label='best', linestyle='--')\n",
    "ax[1].set_xlabel(\"Epoch\")\n",
    "ax[1].set_ylabel(\"Accuracy\")\n",
    "#ax[1].set_xticks(epoch_axis)\n",
    "ax[1].grid()\n",
    "ax[1].legend()\n",
    "fig.suptitle(f\"Training on Hatemoji averaged across {N_SEEDS} seeds\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"figures/epochs_{N_SEEDS}seeds\")\n",
    "plt.show()\n",
    "\n",
    "print(f\"Best (mean) validation accuracy: {np.mean(best_val_accs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only consider best epochs\n",
    "\n",
    "def plot_errorbar(ax, name: str, data, color):\n",
    "    bottom = torch.min(data)\n",
    "    middle = torch.mean(data)\n",
    "    height = torch.max(data) - middle\n",
    "    #err = np.abs(torch.tensor([torch.min(data), torch.max(data)]) - torch.mean(data)).unsqueeze(1)\n",
    "    #ax.errorbar(name, torch.mean(data), yerr=err, fmt='o', color=color, capsize=12)\n",
    "    ax.bar(name, middle-bottom, bottom=bottom, color=color, width=0.5, ec='k')\n",
    "    ax.bar(name, height, bottom=middle, color=color, width=0.5, ec='k')\n",
    "    ax.scatter([name for _ in range(len(data))], data, alpha=0.5, color='k')\n",
    "\n",
    "fig, ax = plt.subplots(1,3, figsize=(10,4))\n",
    "ax[0].hist(best_epochs, bins=np.arange(1, N_EPOCHS+1), ec='k')\n",
    "ax[0].set_xlabel(\"Epoch\")\n",
    "ax[0].set_ylabel(\"Count\")\n",
    "\n",
    "plot_errorbar(ax[1], 'Train accuracy', torch.Tensor(best_train_accs), 'indianred')\n",
    "plot_errorbar(ax[1], 'Validation accuracy', torch.Tensor(best_val_accs), 'indianred')\n",
    "\n",
    "plot_errorbar(ax[2], 'Train loss', torch.Tensor(best_train_losses), 'darkorange')\n",
    "plot_errorbar(ax[2], 'Validation loss', torch.Tensor(best_val_losses), 'darkorange')\n",
    "\n",
    "fig.suptitle(\"Best epochs from 10 seeds\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"figures/best_epochs_from_10seeds\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "def eval_confusion(model, test_dataloader):\n",
    "    progress_bar = tqdm(range(len(test_dataloader)))\n",
    "    model.eval()\n",
    "    confusion_metric = evaluate.load(\"confusion_matrix\")\n",
    "\n",
    "    for batch in test_dataloader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "\n",
    "        logits = outputs.logits\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "\n",
    "        confusion_metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "\n",
    "        progress_bar.update(1)\n",
    "\n",
    "    confusion_matrix = confusion_metric.compute()[\"confusion_matrix\"]\n",
    "\n",
    "    return np.array(confusion_matrix)\n",
    "\n",
    "model.load_state_dict(best_model)\n",
    "confusion_matrix = eval_confusion(model, test_dataloader)\n",
    "confusion_matrix_normalized = confusion_matrix.astype('float') / confusion_matrix.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=confusion_matrix_normalized, display_labels=('not hate','hate'))\n",
    "disp.plot(cmap=plt.cm.binary)\n",
    "print(confusion_matrix_normalized)\n",
    "\n",
    "plt.savefig(f\"hatemoji_confusion.png\", dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
